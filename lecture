Lec 12   Memory Networks
摘要：
    主要是讲了Memory Networks，记忆网络使用推理组件，结合长期记忆，我们研究如何把他们结合起来使用。由于在QA的背景下，长期记
忆表现很好，所以此次研究依然是基于QA的背景。
Memory Networks:http://blog.csdn.net/u011274209/article/details/53384232?ref=myread（讲Memory Networks不错的博客）
    给定一个记忆长度为m的序列（索引是mi）,有如下四个部分：
    I:将输入转化为内部特征表示(可以预处理，也可以编码为内部特征表示)
    G:在新输入的基础上更新旧记忆。（把I(x)存储在slot中，slot可以通过函数选择，不过函数怎么确定？一般G只更新新输入，如果它更复杂，可以更新以前的记忆）
    O:输出一个新的向量（计算执行一个好的反应的相关记忆）
    R:将输出转化为所需要的格式
    
    比如：
    输入一个序列x
    I：I(x)
    G:mi=G(mi,I(x),m)
    
    
    基本模型：
    Memory Networks的实例化，称之为MemNNs，这里用文本式的输入输出实现此过程。
    I：输入一个句子，它可以是一个陈述句，也可以是待回答的一个问题，文本存储在其原始的下一个可用内存槽中。也就是说，S（x）存在下一个空的slot N中，
    mN=x（这里不太懂，S（x），mN，x分别代表？）,N=N+1。G仅仅更新新的记忆，不更新旧的记忆。更复杂的G在后面的部分提到。
    关键部分在于O和R，倘若给定句子x，O输出特征，找最适合的K个记忆模块，迭代遍历得到最适合里面的最大的K。设置k到2，当k=1，支持记忆的最高得
分： 当k=2时，得到的最高得分是： sO表示得分函数。R： W是字典里所有单词的集合。sR是得分匹配函数。最后，得到O的输出（这些公式需要进一步理解，感觉
自己理解的不是特别清楚）还有一些对sO和sR的公式的解释。
    如下是简单的模拟QA： 训练：给定需要的输入和响应，是一个监督学习。当然，这是限于训练数据，对测试数据并不是这样。训练过程，已经可以计算出上述O1、O2
    的最大值，这里使用margin ranking loss和SGD，最小化模型参数Uo和Ur,他们出现在。 f¯、f¯′分别是第一和第二个支撑记忆，
r¯是正确答案。每一步 SGD我们对f，f，r进行采样而不是计算每个训练样本的总和。（应该理解最小化和上述函数的衔接）   

Lec 13  CNN (for NLP) RNNs
    RNN一般只能获得符合语法规则的短语的向量，对于RecursiveNN，需要依赖parser将句子进行解析，获得语法树结构；而CNN考虑的是能否为所有可能的短语组合成
向量，不在乎是否符合语法，自然也就不需要parser。 摘要：我们建立CNN，被称为DCNN（动态CNN），来实现语义句子建模。该网络使用动态的最大k值池化化，是一个
线性序列的全局池化操作。网络处理输入是变长的序列，并促使句子的特征图像能准确地获取短期和长期关系。（这后半句话不是特别懂）
    论文中CNN加入池化操作形成DCNN，然后提出了TDNN。 宽卷积：一维卷积是一个权重向量m(m x 1维)和一个序列S（s x 1维之间的操作。m是卷积的过滤器，具体操
作：s是一个输入句子，有i个单词，通过内积m和s中m-gram（即s中每m个单词为一组），得到一个新的序列c:m=5 左图是当s大于等于m的时候，m=5，而且j大于等于m，
小于等于从，所以是从c5开始的，每5个单词形成一个新的c（s-m+1 x 1维）。右图是对s和m长度没有明确的要求，产生的序列c（s+m-1 x 1维），超出范围的c都作0处
理。（所谓宽卷积指的是在卷积过程中对输入的边缘进行PADDING补零。这样得到的输出会比输入的尺寸更大，而不会使原始的边缘信息丢失。）m-gram中的m，应该小于
等于权重的维度m。 

时延神经网络（TDNN）:
   用于音素识别。序列s和权重m同上。s被认为有时间维度，卷积操作作用于时间维度，s是d x s维，所以其中的Sj是一个向量，不是一个值。同样地，m是d x m
维，多个卷积层可能会被堆积起来生成的序列c作为下一层的输入。
   
MAX-TDNN是一种基于TDNN的句子模式结构。卷积后的结果应用到序列s中，s的每一列是一个特征向量Wi，经过MAX-TDNN后，得到新的序列c,每行选出最大值，得到d x 1
维的向量，得到相关性最大的特征，作为一个分类的输入连接层。MAX-TDNN对单词的序列很敏感，但是不依赖解析树，但是还有其他一些局限性。 

DCNN:
(相关博客：http://blog.csdn.net/liuchonge/article/details/67638232)
Wide convolution:定义每个单词是d维，所以
组成的序列s是d x s维的，（d是训练过程中应该要优化的），随着下面一层矩阵的激活，得到过滤器m(d x s维)，（下一层大概是图上的下面一层？），d和m都是超参数，
如果采用的是一维卷积，得到的是d x (s+m-1)维。
k-max pooling:采用的是基于MAX-TDNN中时间维度的max pooling，不同于一般的池化。传统卷积神经网络中所用到的pooling方法一般是max-pooling，且poolingsize是固定不变的。本文通过k-max-pooling的方法可以在高层获取句子中位置较远的词语之间的联系。本层的作用是将卷
积的输出结果在句长的维度上进行pooling，取出其最大的k个值且保留其相对位置关系。且k的值是依据具体公式动态选取的。
folding：折叠操作，该层不会增加参数数量，简单的将两个维度进行求和，使输入在embed_size维度上减半。（DCNN并没有完全看懂）
